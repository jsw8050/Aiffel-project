{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: https://jeinalog.tistory.com/13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. 전이학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-1. 전이학습의 개념\n",
    "- 높은 정확도를 비교적 짧은 시간 내에 달성 가능\n",
    "- 이미 학습되어 있는 패턴 활용\n",
    "- 사전학습된 모델 이용 (내가 풀고자 하는 문제와 비슷하면서 사이즈가 큰 데이터로 이미 학습이 되어 있는 모델 import)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-2. CNN, 합성곱 신경망\n",
    "- Convolutional base: 합성곱층과 pooling층이 여러겹 쌓여있는 부분\n",
    "    - 이미지로부터 특징 효과적으로 추출하는 것이 목표\n",
    "- Classifier: 완전 연결 계층으로 이루어짐\n",
    "    - 추출된 특징을 잘 학습해서 이미지에 알맞은 카테고리로 분류하는 것이 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-3. 사전 학습된 모델을 내 프로젝트에 맞게 재정의\n",
    "- 1. 원래 모델의 classifier 삭제\n",
    "- 2. 내 목적에 맞는 classifer 추가\n",
    "- 3. 파인튜닝 전략\n",
    "    - 전체 모델 새로 학습: 이 경우에는 사전학습 모델의 구조만 사용한다. 모델을 완전히 새로 학습시켜야하므로, 큰 사이즈의 데이터셋과 좋은 컴퓨팅 연산 능력이 있을 때 적합\n",
    "    - Convolutional base의 일부분은 고정시킨 상태로, 나머지 계층과 classifier만 새로 학습: 데이터셋의 크기에 따라 얼마나 많은 계층을 새로 학습시킬지 달라지는데, 데이터의 양이 많을수록 더 많이 새로 학습시키고, 데이터의 양이 적을수록 학습시키는 부분을 적게 한다.\n",
    "    - Convolutional base는 고정시키고, classifier만 새로 학습시키는 것이다. 이 경우는 convolutional base는 건들지 않고 그대로 두면서 특징 추출 매커니즘으로 활용하고, classifier만 재학습시키는 방법이다. 컴퓨팅 연산능력이 부족하거나 데이터셋이 작을 때 고려"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내가 사용할 모델은 ILSVRC에서 쓰인 모델로 큰 사이즈의 데이터셋인 반면, 새로 학습 시킬 데이터는 약 2만 장으로 비교적 매우 작다. 따라서 가져오는 모델은 이미지에서 특징을 추출하는 매커니즘으로 활용하고, Classifier만 새로 학습시키는 전략이 적절하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-4. 전이학습 과정\n",
    "1) 사전학습 모델 선택\n",
    "\n",
    "2) 내 문제가 데이터 크기-유사성 그래프에서 어떤 부분에 속하는지 알아보기\n",
    "\n",
    "3) 내 모델 fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1-5. Classifer\n",
    "- Convolutional base: 이미지로부터 특징 추출하는 부분\n",
    "    - 낮은 레벨의 계층: input에 가까운 계층으로, 이미지에서 주로 일반적인(general) 특징을 추출\n",
    "    - 높은 레벨의 계층: output에 가까운 계층으로, 보다 구체적이고 특유한 특징을 추출\n",
    "- Classifier: 추출된 특징을 입력받아서 최종적으로 이미지 카테고리 결정하는 부분\n",
    "    - Fully-connected layers: 완전 연결 계층을 쌓은 후 마지막에 소프트맥스 활성화함수(softmax activated layer) 계층을 놓는 것\n",
    "    - Global average pooling, 평균 풀링: Fully-connected layer 대신에 평균 풀링계층을 추가하고, 그 결과값을 바로 소프트맥스 계층과 연결\n",
    "    - Linear support vector machines, 선형 서포트 벡터 머신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. 사전학습모델 VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model VGG16\n",
    "base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,  # VGG 모델 불러오기\n",
    "                                         include_top=False,      # output에 가까운 높은 레벨에 있는 3개 FC 레이어는 제외하고 불러와야해서 False 옵션\n",
    "                                         weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch.shape # 학습 데이터 원래 사이즈 확인\n",
    "\n",
    "# TensorShape([32, 160, 160, 3])\n",
    "# [이미지 장수, (이미지 크기*크기), 채널 수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 배치 넣기\n",
    "feature_batch = base_model(image_batch)\n",
    "feature_batch.shape\n",
    "\n",
    "# TensorShape([32, 5, 5, 512])\n",
    "# 이미지 개수 동일, 사이즈 감소, 채널 증가\n",
    "# 특징 벡터!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조\n",
    "base_model.summary()\n",
    "\n",
    "# dense 블록이 빠진 이유는 전이학습을 수해앟며 새로 학습시킬 것이기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. VGG16 끝단에 classifier 레이어 붙여서 원하는 구조의 분류 모델\n",
    "- VGG16이 출력하는 벡터의 shape: (32, 5, 5, 512): 3차원\n",
    "- classifier를 구성하려면 fully connected 레이어로 구성해야 함: 반드시 1차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "Original image shape: (2, 2)\n",
      "\n",
      "Flattened image:\n",
      " [1 2 3 4]\n",
      "Flattened image shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "# flatten\n",
    "import numpy as np\n",
    "\n",
    "image = np.array([[1, 2],\n",
    "                  [3, 4]])\n",
    "\n",
    "flattened_image = image.flatten()\n",
    "\n",
    "print(\"Original image:\\n\", image)\n",
    "print(\"Original image shape:\", image.shape)\n",
    "print()\n",
    "print(\"Flattened image:\\n\", flattened_image)\n",
    "print(\"Flattened image shape:\", flattened_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3-1. 2D Global average pooling\n",
    "- 3차원의 벡터가 있을 때, 겹겹이 쌓여있는 2차원 배열을 평균을 취한 후 하나의 값으로 축소시키는 기법\n",
    "- Global Average Pooling을 거치게 되면 (Height, Width, Depth)의 shape를 가지는 3차원 이미지의 shape가 (1, 1, Depth)로 바뀐다. 즉, (Depth)의 shape로, 1차원 벡터로 볼 수 있다.\n",
    "-  VGG16이 출력한 벡터가 [32, 5, 5, 512]였으니, Global Average Pooling을 취하면 벡터의 shape가 [32, 512]가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Average Pooling 계층을 만드는 코드\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)\n",
    "# (32, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense 레이어 붙여주면 새로운 classifier 완성\n",
    "dense_layer = tf.keras.layers.Dense(512, activation='relu')\n",
    "prediction_layer = tf.keras.layers.Dense(2, activation='softmax')\n",
    "\n",
    "# feature_batch_averag가 dense_layer를 거친 결과가 다시 prediction_layer를 거치게 되면\n",
    "prediction_batch = prediction_layer(dense_layer(feature_batch_average))  \n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False # 학습 안 시킬거라서 train 변수 off \n",
    "# VGG16 base_model 에 입력되어 특징이 추출된 다음, 특징벡터는 global_average_layer를 거쳐 마지막에 prediction_layer까지 통과하며 강아지인지, 고양이인지 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. 최종 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,  # VGG16 내 레이어들 간결하게 표현\n",
    "  global_average_layer,\n",
    "  dense_layer,\n",
    "  prediction_layer\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-4-1. VGG16 기반 이미지 분류와 직접 만든 분류 모델 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아직 학습되지 않은 상태\n",
    "validation_steps=20\n",
    "loss0, accuracy0 = model.evaluate(validation_batches, steps = validation_steps)\n",
    "\n",
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5   # 이번에는 이전보다 훨씬 빠르게 수렴되므로 5Epoch이면 충분\n",
    "\n",
    "history = model.fit(train_batches,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 과정 그래프\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 예측 결과\n",
    "for image_batch, label_batch in test_batches.take(1):\n",
    "    images = image_batch\n",
    "    labels = label_batch\n",
    "    predictions = model.predict(image_batch)\n",
    "    pass\n",
    "\n",
    "predictions # 확률값 0과 1 사이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction 값들을 실제 추론한 라벨(고양이:0, 강아지:1)로 변환\n",
    "import numpy as np\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for idx, (image, label, prediction) in enumerate(zip(images, labels, predictions)):\n",
    "    plt.subplot(4, 8, idx+1)\n",
    "    image = (image + 1) / 2\n",
    "    plt.imshow(image)\n",
    "    correct = label == prediction\n",
    "    title = f'real: {label} / pred :{prediction}\\n {correct}!'\n",
    "    if not correct:\n",
    "        plt.title(title, fontdict={'color': 'red'})\n",
    "    else:\n",
    "        plt.title(title, fontdict={'color': 'blue'})\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 확인\n",
    "count = 0\n",
    "for image, label, prediction in zip(images, labels, predictions):\n",
    "    correct = label == prediction\n",
    "    if correct:\n",
    "        count = count + 1\n",
    "\n",
    "print(count / 32 * 100) # 약 95% 내외"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5. 모델을 save 하고, 다시 load 해와서 사용하는 방법\n",
    "- 저장: save_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = os.getenv(\"HOME\") + \"/workplace/aiffel/Exploration/04. 이미지 분류/cat_vs_dog/checkpoint\"\n",
    "checkpoint_file_path = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "\n",
    "if not os.path.exists('checkpoint_dir'):\n",
    "    os.mkdir('checkpoint_dir')\n",
    "    \n",
    "model.save_weights(checkpoint_file_path)     # checkpoint 파일 생성\n",
    "\n",
    "if os.path.exists(checkpoint_file_path):\n",
    "  print('checkpoint 파일 생성 OK!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-6. 학습된 모델에 원하는 이미지를 입력해 예측 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 저장\n",
    "img_dir_path = os.getenv(\"HOME\") + \"/aiffel/cat_vs_dog/images\"\n",
    "os.path.exists(img_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 플로우 모델에 입력\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터로 이미지 사이즈 설정\n",
    "IMG_SIZE = 160\n",
    "dog_image_path = os.path.join(img_dir_path, 'my_dog.jpeg')\n",
    "\n",
    "dog_image = load_img(dog_image_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "dog_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배열자료형 변환\n",
    "dog_image = img_to_array(dog_image).reshape(1, IMG_SIZE, IMG_SIZE, 3)\n",
    "dog_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고양이라면 [1.0, 0.0], 강아지라면 [0.0, 1.0]에 가까운 확률 분포가 예측\n",
    "prediction = model.predict(dog_image)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 함수\n",
    "def show_and_predict_image(dirpath, filename, img_size=160):\n",
    "    filepath = os.path.join(dirpath, filename)\n",
    "    image = load_img(filepath, target_size=(img_size, img_size))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    image = img_to_array(image).reshape(1, img_size, img_size, 3)\n",
    "    prediction = model.predict(image)[0]\n",
    "    cat_percentage = round(prediction[0] * 100)\n",
    "    dog_percentage = round(prediction[1] * 100)\n",
    "    print(f\"This image seems {dog_percentage}% dog, and {cat_percentage}% cat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수에 이미지 넣기\n",
    "filename = 'my_dog.jpeg'\n",
    "\n",
    "show_and_predict_image(img_dir_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'my_cat.jpeg'\n",
    "\n",
    "show_and_predict_image(img_dir_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
