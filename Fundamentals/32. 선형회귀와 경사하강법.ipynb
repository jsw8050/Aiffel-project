{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 연립방정식의 근사해\n",
    "추세선은 주어진 데이터에 가장 잘 맞는 직선을 긋는 것이나 모든 점을 지나는 것은 불가능하다. 즉, 주어진 연립방정식이 해를 갖지 않는다. 이 경우 두 가지 접근법이 있다.\n",
    "\n",
    "첫째, 최소제곱법을 이용해 가장 가까운 근사해를 찾는다.\n",
    "\n",
    "둘째, 선형회귀를 이용해 근사해를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 최소제곱해와 정규방정식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.428571428571433 78.99999999999999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.array([2, 4, 8])\n",
    "A = np.array([[1]*len(x), x]).T\n",
    "y = np.array([81, 93, 97])\n",
    "\n",
    "c, m = np.linalg.lstsq(A, y, rcond=None)[0] # 최소자승 해 풀기 (Compute the Least-squares solution)\n",
    "\n",
    "print(m, c)\n",
    "\n",
    "plt.plot(x, y, 'o', label='Original data', markersize=10)\n",
    "plt.plot(x, m*x + c, 'r', label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b_c := A\\hat{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $A^T(b-A\\hat{x})=0 \\iff A^T A\\hat{x}=A^T b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 정규방정식의 해 표현\n",
    "$rank(A)=n+1$, 즉 full rank인 경우 (대부분의 경우 A는 full rank)를 갖는다. 피쳐가 서로 독립적이면 무조건 그렇고, 그렇지 않더라도 피쳐 엔지니어링(유사한 피쳐 제거, PCA 등의 차원축소 등)을 거치면 대부분 그렇게 됩니다.\n",
    "\n",
    "이 경우에는 $A^T A$의 역행렬이 존재하기 때문에 다음과 같이 간단하게 최소제곱해를 얻을수  있습니다.\n",
    "\n",
    "$x = (A^T A)^{-1} A b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 14]\n",
      " [14 84]]\n",
      "[ 271 1310]\n",
      "[[ 1.5        -0.25      ]\n",
      " [-0.25        0.05357143]]\n",
      "[79.          2.42857143]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([2, 4, 8])\n",
    "A = np.array([[1]*len(x), x]).T\n",
    "y = np.array([81, 93, 97])\n",
    "\n",
    "print(np.matmul(A.T, A)) # 행렬곱\n",
    "print(np.matmul(A.T, y)) # 행렬곱\n",
    "print(np.linalg.inv(np.matmul(A.T, A)))\n",
    "# [ 3/2, -1/4]\n",
    "# [-1/4, 3/56]\n",
    "print(np.matmul(np.linalg.inv(np.matmul(A.T, A)), np.matmul(A.T, y)))\n",
    "# (79, 17/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (         0/      2000) cost: 8206.333333, W:   0.000000, b:  0.000000\n",
      "Epoch (       100/      2000) cost: 555.446152, W:  10.752730, b: 29.440640\n",
      "Epoch (       200/      2000) cost: 239.027498, W:   7.825787, b: 46.866718\n",
      "Epoch (       300/      2000) cost: 106.006292, W:   5.928016, b: 58.165432\n",
      "Epoch (       400/      2000) cost:  50.084681, W:   4.697540, b: 65.491290\n",
      "Epoch (       500/      2000) cost:  26.575446, W:   3.899725, b: 70.241227\n",
      "Epoch (       600/      2000) cost:  16.692253, W:   3.382437, b: 73.320989\n",
      "Epoch (       700/      2000) cost:  12.537396, W:   3.047039, b: 75.317845\n",
      "Epoch (       800/      2000) cost:  10.790710, W:   2.829573, b: 76.612565\n",
      "Epoch (       900/      2000) cost:  10.056410, W:   2.688573, b: 77.452036\n",
      "Epoch (      1000/      2000) cost:   9.747713, W:   2.597151, b: 77.996332\n",
      "Epoch (      1100/      2000) cost:   9.617938, W:   2.537875, b: 78.349242\n",
      "Epoch (      1200/      2000) cost:   9.563381, W:   2.499442, b: 78.578062\n",
      "Epoch (      1300/      2000) cost:   9.540445, W:   2.474522, b: 78.726424\n",
      "Epoch (      1400/      2000) cost:   9.530803, W:   2.458365, b: 78.822619\n",
      "Epoch (      1500/      2000) cost:   9.526750, W:   2.447889, b: 78.884990\n",
      "Epoch (      1600/      2000) cost:   9.525046, W:   2.441097, b: 78.925430\n",
      "Epoch (      1700/      2000) cost:   9.524329, W:   2.436692, b: 78.951650\n",
      "Epoch (      1800/      2000) cost:   9.524028, W:   2.433837, b: 78.968651\n",
      "Epoch (      1900/      2000) cost:   9.523901, W:   2.431985, b: 78.979674\n",
      "W:   2.430785\n",
      "b:  78.986821\n",
      "result :  [83.848391   88.70996105 98.43310117]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 관측된 x, y를 데이터로 삼습니다.\n",
    "x_train = np.array([2, 4, 8])\n",
    "y_train = np.array([81, 93, 97])\n",
    "\n",
    "# 학습해야 할 파라미터 W, b를 놓고\n",
    "W, b = np.array([0.0, 0.0])\n",
    "n_data = len(x_train)\n",
    "epochs = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 루프를 돌면서 \n",
    "for i in range(epochs):\n",
    "    hypothesis = x_train * W + b\n",
    "    cost = np.sum((hypothesis - y_train) ** 2) / n_data\n",
    "    # gradient를 계산하여\n",
    "    gradient_w = np.sum((W * x_train - y_train + b) * 2 * x_train) / n_data\n",
    "    gradient_b = np.sum((W * x_train - y_train + b) * 2) / n_data\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('Epoch ({:10d}/{:10d}) cost: {:10f}, W: {:10f}, b:{:10f}'.format(i, epochs, cost, W, b))\n",
    "\n",
    "    # 그래디언트 반대 방향으로 파라미터를 업데이트합니다. \n",
    "    W -= learning_rate * gradient_w\n",
    "    b -= learning_rate * gradient_b\n",
    "\n",
    "\n",
    "print('W: {:10f}'.format(W))\n",
    "print('b: {:10f}'.format(b))\n",
    "print('result : ', x_train * W + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 회귀 모델을 학습시킬 때: 주로 손실함수를 MSE(Mean Squared Error)로 택해 최소화시키는 방식으로 학습\n",
    "- 학습방법은 경사하강법(GD, Gradient Descent)을 사용\n",
    "- 손실함수를 최소화: Gradient 벡터의 크기가 가장 빠르게 감소하는 방향으로 가중치를 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 경사하강법\n",
    "손실함수(Loss function)을 정의하고 그 최솟값을 찾는 작업에 사용된다.\n",
    "- 그래디언트 벡터는 레벨셋에 수직이고 그래디언트 방향이 함수가 가장 급하게 증가하는 방향이다.\n",
    "- 경사하강법(Gradient descent)은 여기에 착안하여 순간순간 그래디언트 벡터의 반대(음의 그래디언트) 방향으로 이동하며 최솟값을 찾아가는 방법이다.\n",
    "- 한번에 너무 많이 점프하면 오버슈팅이 일어날 수 있고, 너무 적게 점프하면 수렴속도가 너무 느려진다.\n",
    "- 이렇게 점프하는 정도를 조절하는 하이퍼파라미터(Hyperparameter)가 학습률(Learning rate)이다.\n",
    "- 학습 초기에는 학습률을 좀더 높게 잡아줬다가 뒤로가면 미세조절을 위해 작게 잡아주는 방식을 많이 택한다.\n",
    "- 경사하강법이 갖는 문제점 중 하나는 컨벡스 문제(Convex problem)이 아닌 경우, 즉, 손실함수가 볼록이 아닐 경우, 최솟값(Global minimum)이 아니라 극솟값(Local minimum)으로 수렴할수 있다.\n",
    "- 실제로는 평지(Plateau, 수학적으로는 saddle point에 해당)가 생겨서 weight 업데이트가 거의 일어나지 않는 현상이 훨씬 문제가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 옵티마이저\n",
    "#### 6-0. 경사하강법(GD, Gradient Descent)\n",
    "뉴럴넷의 weight들을 모아놓은 벡터를 w라고 했을 때, 뉴럴넷에서 내놓는 결과값과 실제 결과값 사이의 차이를 정의하는 손실함수 C(w)의 값을 최소화하기 위해 gradient의 반대 방향으로 일정 크기만큼 이동해내는 것을 반복하여 손실함수의 값을 최소화하는 w의 값을 찾는 알고리즘을 경사하강법(GD)이라 한다.\n",
    "\n",
    "$w_{t+1}=w_t−\\eta\\nabla_{w_t} C(w_t)$\n",
    "\n",
    "η가 미리 정해진 걸음의 크기(step size)로서, 위에서 언급한 하이퍼파라미터인 학습률(Learning rate)이구요. 보통 0.01~0.001 정도의 적당한 크기를 사용하고, 보통 학습 초기에는 학습률을 좀더 높게 잡아줬다가 뒤로가면 미세조절을 위해 작게 잡아주는 방식을 많이 택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-1. 확률적 경사하강법(SGD, Stochastic Gradient Descent)\n",
    "- GD에서는 한번 step을 내딛을 때 전체 훈련 데이터에 대해 손실함수를 계산해야 하므로 너무 많은 계산량이 필요해서 속도가 엄청 느리다. 이를 방지하기 위해 보통 확률적 경사하강법(SGD)를 사용한다.\n",
    "- 손실함수를 계산할 때 전체 데이터(total batch) 대신 일부 조그마한 데이터의 모음(mini-batch)에 대해서만 손실함수를 계산하기 대문에 GD보다 다소 부정확할 수는 있지만, 훨씬 계산 속도가 빠르기 때문에 같은 시간에 더 많은 step을 갈 수 있으며 여러 번 반복할 경우 보통 batch의 결과와 유사한 결과로 수렴한다.\n",
    "- GD에 비해 빠르다는거지 앞으로 소개할 방법들에 비하면 굉장히 느리다. \n",
    "- SGD를 사용할 경우 GD에 비해 local minima에 빠지지 않고 더 좋은 방향으로 수렴할 가능성도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6-2. 모멘텀(Momentum)\n",
    "- Momentum 방식은 말 그대로 SGD를 통해 이동하는 과정에 일종의 '관성'을 주는 것이다. 현재 gradient를 통해 이동하는 방향과는 별개로, 과거에 이동했던 방식을 기억하면서 그 방향으로 일정 정도를 추가적으로 이동하는 방식이다.\n",
    "\n",
    "$v_t$는 time step t에서의 이동벡터를 의미하다.\n",
    "\n",
    "$v_{t+1} = m v_t + \\eta \\nabla_{w_t} C(w_t)$\n",
    "$w_t=w_{t-1}−v_t$\n",
    "\n",
    "이 때, m은 얼마나 momentum을 줄 것인지에 대한 관성항 값으로, 보통 0.9 정도의 값을 사용해요. 식을 살펴보면 과거에 얼마나 이동했는지에 대한 이동 항 vt에 관성항 값을 곱하고 gradient 벡터에 step size η를 곱해서 더해준다.\n",
    "\n",
    "관성 효과로 인해 양(+) 방향, 음(-) 방향 순차적으로 일어나는 지그재그 현상이 줄어든다는 거예요. 진동을 하더라도 중앙으로 가는 방향에 힘을 얻기 때문에 SGD에 비해 상대적으로 빠르게 이동할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-3. 아다그래드(Adagrad)\n",
    "- Adagrad는 변수들을 업데이트 할때 각각의 변수마다 step size를 다르게 설정해서 이동하는 방식\n",
    "- '지금까지 많이 변화하지 않은 변수들은 step size를 크게 하고, 지금까지 많이 변화했던 변수들은 step size를 작게 하자' \n",
    "- 자주 등장하거나 변화를 많이 한 변수들의 경우 optimum에 가까이 있을 확률이 높기 때문에 작은 크기로 이동하면서 세밀한 값을 조정하고, 적게 변화한 변수들은 optimum 값에 도달하기 위해서는 많이 이동해야할 확률이 높기 때문에 먼저 빠르게 loss 값을 줄이는 방향으로 이동하려는 방식\n",
    "- 특히 word2vec이나 GloVe 같이 word representation을 학습시킬 경우 단어의 등장 확률에 따라 variable의 사용 비율이 확연하게 차이나기 때문에 Adagrad와 같은 학습 방식을 이용하면 훨씬 더 좋은 성능을 거둘 수 있다.\n",
    "\n",
    "$G_{t+1} = G_t + (\\nabla_{w_t} C(w_t))^2$\n",
    "\n",
    "$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t+\\epsilon}} \\cdot \\nabla_{w_t} C(w_t)$\n",
    "\n",
    "$G_t+1$을 업데이트하는 식에서 제곱은 element-wise 제곱을 의미하며, $W_t+1$을 업데이트하는 식에서도 ⋅ 은 element-wise한 연산을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: element-wise 제곱 (요소별 연산) ex. 행렬 덧셈 빽셈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adagrad를 사용하면 학습을 진행하면서 굳이 step size decay등을 신경써주지 않아도 된다는 장점이 있다. 보통 Adagrad에서 step size로는 0.01 정도를 사용한 뒤, 그 이후로는 바꾸지 않는다.\n",
    "- 반면, Adagrad에는 학습을 계속 진행하면 step size가 너무 줄어든다는 문제점이 있다. G에는 계속 제곱한 값을 넣어주기 때문에 G의 값들은 계속해서 증가하기 때문에, 학습이 오래 진행될 경우 step size가 너무 작아져서 결국 거의 움직이지 않게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-4. RMSProp\n",
    " Adagrad의 단점을 해결하기 위한 방법이다. Adagrad의 식에서 gradient의 제곱값을 더해나가면서 구한 Gt 부분을 합이 아니라 지수평균(Exponential Average)으로 바꾸어서 대체한 방법이다.\n",
    " \n",
    " 이렇게 대체를 할 경우 Adagrad처럼 Gt가 무한정 커지지는 않으면서 최근 변화량의 변수간 상대적인 크기 차이는 유지할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6-5. 아담(Adam)\n",
    "Adam은 RMSProp과 Momentum 방식을 합친 것 같은 알고리즘이다. 이 방식에서는 Momentum 방식과 유사하게 지금까지 계산해온 기울기의 지수평균을 저장하며, RMSProp과 유사하게 기울기의 제곱값의 지수평균을 저장한다.\n",
    "\n",
    "다만, Adam에서는 m과 v가 처음에 0으로 초기화되어 있기 때문에 학습의 초반부에서는 mt, vt가 0에 가깝게 bias 되어있을 것이라고 판단하여 이를 unbiased 하게 만들어주는 작업을 거친다.\n",
    "\n",
    "보통 β1로는 0.9, β2로는 0.999, ϵ으로는 $10^-8$ 정도의 값을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
