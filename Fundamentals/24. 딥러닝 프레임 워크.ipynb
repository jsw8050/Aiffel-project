{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 신경망 구성\n",
    "우리 뇌 속의 신경망 구조에 착안해서 퍼셉트론(Perceptron)이라는 형태를 제안하며 이를 연결한 형태를 인공신경망(Artificial Neural Network)이라고 부르기 시작했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. MNIST Revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 1s 778us/step - loss: 0.4931 - accuracy: 0.8828\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 1s 752us/step - loss: 0.2292 - accuracy: 0.9351\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 1s 778us/step - loss: 0.1797 - accuracy: 0.9482\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 808us/step - loss: 0.1506 - accuracy: 0.9567\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 1s 756us/step - loss: 0.1301 - accuracy: 0.9632\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 1s 752us/step - loss: 0.1147 - accuracy: 0.9679\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 1s 759us/step - loss: 0.1021 - accuracy: 0.9712\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 1s 761us/step - loss: 0.0914 - accuracy: 0.9742\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 1s 762us/step - loss: 0.0829 - accuracy: 0.9769\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 821us/step - loss: 0.0750 - accuracy: 0.9796\n",
      "313/313 - 0s - loss: 0.1015 - accuracy: 0.9701\n",
      "test_loss: 0.101475290954113 \n",
      "test_accuracy: 0.9700999855995178\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 3개의 레이어로 구성되된 퍼셉트론을 나타냅니다. 위에서 보았던 예시 코드와도 동일합니다. 은닉층에는 H개의 노드이, 출력층에는 K개의 노드가 존재하는 인공신경망을 표현한 것입니다. (+1 부분은 bias를 뜻하는 부분이므로 이전 레이어와의 연결이 없습니다. ) 위의 코드에서는 H=50, K=10, 그리고 입력층 노드 개수 d=784로 정의되었습니다.\n",
    "https://funnypr.tistory.com/entry/The-Basic-Artificial-Neuron-Bias-neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 이미지를 보면 입력값이 있는 입력층(input layer), 최종 출력값이 있는 출력층(output layer), 그리고 그 사이에 있는 층인 은닉층(hidden layer)이 있습니다. 보통 입력층과 출력층 사이에 몇 개의 층이 존재하든 모두 은닉층이라고 부릅니다.\n",
    "\n",
    "- 보통 그림으로 인공신경망을 표현할 때에는 노드를 기준으로 레이어를 표시해서 3개의 레이어라고 생각할 수 있지만, 실제로는 총 2개의 레이어를 가졌습니다. 레이어 개수를 셀 때에는 노드와 노드 사이의 연결하는 부분이 몇 개 존재하는지 세면 보다 쉽게 알 수 있습니다.\n",
    "\n",
    "- 이렇게 인공신경망이 어떻게 생겼는지 대략 알아봤는데요. 인공신경망 중에서도 위의 이미지처럼 2개 이상의 레이어를 쌓아서 만든 것을 보통 다층 퍼셉트론(Multi-Layer Perceptron; MLP)라고 부릅니다. 그리고 입력층, 출력층을 제외한 은닉층이 많아지면 많아질수록 인공신경망이 DEEP 해졌다고 이야기합니다.\n",
    "\n",
    "- 우리가 지금 알아보려고 하는 딥러닝이 바로 이 인공신경망이 DEEP해졌다는 뜻에서 나온 단어입니다. 그래서 우리가 하려는 딥러닝은 충분히 깊은 인공신경망을 활용하며 이를 보통 다른 단어로 DNN(Deep Neural Network)라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fully-Connected Nertaul Network**\n",
    "\n",
    "MLP의 다른 용어로, Fully-Connnected Nerual Network는 서로 다른 층에 위치한 노드 간에는 연결 관계가 존재하지 않으며, 인접한 층에 위치한 노드들 간의 연결만 존재한다는 의미를 내포합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3. Parameters/Weights\n",
    "- 입력층-은닉층, 은닉층-출력층 사이에는 각가 행력이 존재한다.\n",
    "- 입력값이 100개, 은닉 노드가 20개라면, 100X20 형태를 가진 행렬이 존재한다.\n",
    "- 출력층이 10개의 노드를 가진다면, 은닉층-출력층 사이에는 20X10 형태를 가진 행렬이 존재한다.\n",
    "- 이 행렬들을 파라미터 혹은 웨이트라고 부른다. 보통 같은 뜻으로 사용되지만, 실제로 파라미터에는 bias 노드도 포함된다.\n",
    "\n",
    "$y = W * X + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP 기반 딥러닝 모델을 Numpy로 다시 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(5, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X = x_train_reshaped[:5]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size=50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)  \n",
    "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.25637516e-01,  1.25647762e+00, -1.29578927e+00, -2.50634033e-01,\n",
       "        1.19123032e+00,  9.54569677e-02, -6.16784358e-01,  4.49809989e-02,\n",
       "        1.19927191e+00,  8.19375163e-01,  6.21815372e-01,  9.81901661e-01,\n",
       "        7.45971979e-01,  1.44222493e+00, -8.92543279e-01,  1.28425893e+00,\n",
       "       -1.33728070e-01,  2.64309347e-01,  9.19180051e-01, -6.11430700e-01,\n",
       "        3.07208499e-01, -2.09713969e-01, -1.01650476e-01,  5.28805574e-01,\n",
       "        6.24988001e-01,  5.74697254e-01, -7.84530234e-03,  2.94126816e-01,\n",
       "       -1.98250552e+00, -2.86091344e-01, -1.13633407e+00, -4.26799364e-01,\n",
       "        1.05108412e+00, -1.66291442e+00,  3.54452132e-01, -6.75015611e-01,\n",
       "        6.18215905e-01,  2.19635502e-01, -1.12104666e-01, -2.98554398e-01,\n",
       "        8.38527240e-01, -5.14232360e-01,  6.76068102e-04, -1.41123751e+00,\n",
       "        1.31662291e+00,  1.85668388e+00,  1.61593909e+00, -4.19229051e-01,\n",
       "       -9.91191995e-01,  8.62365820e-01])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 데이터의 은닉층 출력\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 활성화 함수와 손실 함수\n",
    "#### 2-1. 활성화 함수 (Activation Functions)\n",
    "딥러닝에서는 이 활성화 함수의 존재가 필수적입니다. 다소 수학적인 이유가 있지만, 간단히 설명하자면 이 활성화 함수는 보통 비선형 함수를 사용합니다. 그리고 이 비선형 함수를 MLP 안에 포함시키면서 모델의 표현력이 좋아지게 됩니다. (정확히는 이 비선형 함수가 포함되지 않으면 한 층을 가진 MLP와 다른 점이 없습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Sigmoid\n",
    "$σ(x)=\\frac{1}{1+e^{−x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39516853 0.77841915 0.21487453 0.43766745 0.76696103 0.52384614\n",
      " 0.35051315 0.51124335 0.76839523 0.69410369 0.65063131 0.72748538\n",
      " 0.67830038 0.80879896 0.29058526 0.78317387 0.46661772 0.56569533\n",
      " 0.71487501 0.3517329  0.57620374 0.44776282 0.47460924 0.62920449\n",
      " 0.65135214 0.63984633 0.49803868 0.57300614 0.121052   0.42896104\n",
      " 0.24299407 0.39489087 0.74098303 0.15937116 0.58769679 0.33737467\n",
      " 0.64981268 0.5546892  0.47200315 0.42591091 0.69815495 0.37420189\n",
      " 0.50016902 0.19603894 0.7886193  0.86490996 0.83423432 0.39670125\n",
      " 0.2706767  0.7031547 ]\n"
     ]
    }
   ],
   "source": [
    "# 위 수식의 sigmoid 함수를 구현해 봅니다.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])  # sigmoid의 출력은 모든 엘리먼트가 0에서 1사이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단점**\n",
    "- Gradient Vanishing 현상이 발생한다.\n",
    "- exp 함수 사용시 비용이 크다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) tanh\n",
    "$tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "\n",
    "- tanh 함수는 함수의 중심값을 0으로 옮겨 sigmoid의 최적화 과정이 느려지는 문제를 해결.\n",
    "- gradient vanishing 문제 존재."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) ReLU\n",
    "$f(x)=max(0,x)$\n",
    "- sigmoid, tanh 함수와 비교 시 학습 빠름.\n",
    "- 연산 비용이 크지 않고, 구현이 매우 간단하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go~\n"
     ]
    }
   ],
   "source": [
    "# 단일 레이어 구현 함수\n",
    "def affine_layer_forward(X, W, b):\n",
    "    y = np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache\n",
    "\n",
    "print('go~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.67301816 -0.08848628  0.23114498 -0.13141477 -0.62625858 -0.80826509\n",
      " -0.20637871 -0.18166718  0.28067251  0.20866228]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됩니다. \n",
    "\n",
    "print(a2[0])  # 최종 출력이 output_size만큼의 벡터가 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 출력인 a2에 softmax 함수 적용하면 입력 x가 10가지 숫자 중 하나일 확률의 형태로 가공\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05833108, 0.10465476, 0.14406984, 0.10025715, 0.06112339,\n",
       "       0.05095221, 0.09301628, 0.09534349, 0.15138491, 0.1408669 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]  # 10개의 숫자 중 하나일 확률이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. 손실함수\n",
    "이렇게 비선형 활성화 함수를 가진 여러 개의 은닉층을 거친 다음 신호 정보들은 출력층으로 전달됩니다. 이 때 우리가 원하는 정답과 전달된 신호 정보들 사이의 차이를 계산하고, 이 차이를 줄이기 위해 각 파라미터들을 조정하는 것이 딥러닝의 전체적인 학습 흐름입니다.\n",
    "\n",
    "이 차이를 구하는데 사용되는 함수는 손실함수(Loss function) 또는 비용함수(Cost function)라고 부릅니다. 대표적으로 다음과 같은 두 가지 손실함수가 존재합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 평균제곱오차 (MSE; Mean Square Error)\n",
    "${MSE} ={\\frac{1}{n}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 교차 엔트로피 (Cross Entropy)\n",
    "Cross Entropy는 두 확률분포 사이의 유사도가 클수록 작아지는 값입니다. 아직 별로 학습되지 않은 현재의 모델이 출력하는 softmax 값 y_hat은 10개의 숫자 각각의 확률이 대부분 0.1 근처를 오가는 정도입니다.\n",
    "\n",
    "모델을 학습하게 되면, yhat이 점점 정답에 가까워지게 됩니다. 정말 그렇게 되는지 다음 스텝에서 살펴 봅시다. 우선은 yhat과 정답을 비교해 봅시다.\n",
    "\n",
    "$E= -\\sum_{i=1}^{n} {t_{i}\\log{y_i}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 라벨을 One-hot 인코딩하는 함수\n",
    "def _change_ont_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_ont_hot_label(Y_digit, 10)\n",
    "t     # 정답 라벨의 One-hot 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05833108 0.10465476 0.14406984 0.10025715 0.06112339 0.05095221\n",
      " 0.09301628 0.09534349 0.15138491 0.1408669 ]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat[0])\n",
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5800257266168627"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 경사하강법\n",
    "- 각 단계에서의 기울기를 구해서 해당 기울기가 가리키는 방향으로 이동\n",
    "- 학습률(learning rate) 이라는 개념을 도입해 기울기 값과 이 학습률을 곱한 만큼만 발걸음을 내딛음\n",
    "- 우리가 발걸음을 잘 내딛는다고 해도 어디서 출발했냐에 따라 산 아래로 내려가는 시간이 빨라질 수도 느려질 수도 있습니다. 이는 parameter의 값들을 어떻게 초기화하는지의 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01166622,  0.02093095,  0.02881397,  0.02005143,  0.01222468,\n",
       "        -0.18980956,  0.01860326,  0.0190687 ,  0.03027698,  0.02817338],\n",
       "       [-0.18783061,  0.01977652,  0.03169325,  0.01933165,  0.01047892,\n",
       "         0.01084571,  0.01944427,  0.01612602,  0.03134881,  0.02878547],\n",
       "       [ 0.01567094,  0.02137537,  0.02472271,  0.02111222, -0.18878449,\n",
       "         0.01301394,  0.01731175,  0.02129398,  0.03113638,  0.02314719],\n",
       "       [ 0.01299947, -0.18016127,  0.02557152,  0.02238328,  0.01203243,\n",
       "         0.00866472,  0.0239688 ,  0.02342641,  0.02490111,  0.02621352],\n",
       "       [ 0.01352884,  0.022916  ,  0.0257529 ,  0.019132  ,  0.01173028,\n",
       "         0.01012606,  0.01612668,  0.02119449,  0.03052594, -0.1710332 ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy    # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0360628 , -0.03096036,  0.06947636,  0.05262649, -0.09020019,\n",
       "        -0.06130265,  0.04813671,  0.05317978,  0.07719846, -0.0820918 ],\n",
       "       [-0.13007638, -0.07472453,  0.08550373,  0.0625303 , -0.05760146,\n",
       "        -0.11954193,  0.05995654,  0.06049101,  0.09026534,  0.02319739],\n",
       "       [-0.0609958 , -0.02513629,  0.07291611,  0.05353231, -0.04192239,\n",
       "        -0.11984459,  0.04977389,  0.05314339,  0.0790756 , -0.06054223],\n",
       "       [-0.11105962,  0.02112713,  0.0619358 ,  0.04410132, -0.07383961,\n",
       "        -0.08555452,  0.04070269,  0.04222913,  0.06717521, -0.00681752],\n",
       "       [-0.04552547, -0.05793425,  0.05047433,  0.03847827, -0.06288583,\n",
       "        -0.02981429,  0.0362366 ,  0.03843387,  0.05482484, -0.02228808],\n",
       "       [-0.01909577, -0.08196351,  0.05616909,  0.04454475, -0.12468372,\n",
       "        -0.02928922,  0.04161549,  0.04481783,  0.06201206,  0.005873  ],\n",
       "       [-0.091195  , -0.03327618,  0.0796791 ,  0.05763538, -0.02191468,\n",
       "        -0.14781003,  0.05434225,  0.05647005,  0.0850254 , -0.03895629],\n",
       "       [-0.03995806, -0.10408414,  0.07788015,  0.05990276, -0.06977588,\n",
       "        -0.04663164,  0.05635806,  0.06085086,  0.08484842, -0.07939053],\n",
       "       [-0.0758069 , -0.00280288,  0.07421136,  0.05454305, -0.08457472,\n",
       "        -0.09533263,  0.05003998,  0.05389456,  0.08152284, -0.05569466],\n",
       "       [-0.10457212,  0.0210224 ,  0.07521836,  0.05465261, -0.10376482,\n",
       "        -0.07794057,  0.04980566,  0.05346553,  0.08294061, -0.05082766],\n",
       "       [-0.10749619, -0.00343048,  0.06328798,  0.0451013 , -0.03830849,\n",
       "        -0.09021991,  0.04226586,  0.04354591,  0.06778727, -0.02253325],\n",
       "       [-0.12569654, -0.06342099,  0.11201993,  0.08255766, -0.08374039,\n",
       "        -0.13553878,  0.07740139,  0.08153699,  0.12093657, -0.06605583],\n",
       "       [-0.13265841, -0.06655576,  0.08022712,  0.05848657, -0.04781992,\n",
       "        -0.08492461,  0.0559694 ,  0.05678581,  0.08491996, -0.00443016],\n",
       "       [-0.0702729 , -0.07636107,  0.07397957,  0.05549714, -0.05863286,\n",
       "        -0.07815502,  0.05252573,  0.05515547,  0.07956491, -0.03330098],\n",
       "       [-0.1259989 , -0.02794706,  0.07307723,  0.05284758, -0.05969526,\n",
       "        -0.07451933,  0.04982031,  0.05120259,  0.07825438, -0.01704155],\n",
       "       [-0.07087941, -0.10238571,  0.0673258 ,  0.05075227, -0.04521642,\n",
       "        -0.0783386 ,  0.0490057 ,  0.05005703,  0.07113452,  0.00854482],\n",
       "       [-0.12855716, -0.06816218,  0.08351058,  0.06074478, -0.02998373,\n",
       "        -0.09704501,  0.05809025,  0.05929677,  0.08837833, -0.02627263],\n",
       "       [-0.07174676, -0.07904195,  0.07591239,  0.05724977, -0.08475907,\n",
       "        -0.08650989,  0.05415976,  0.0565704 ,  0.08173148, -0.00356613],\n",
       "       [-0.13290446, -0.01435241,  0.09163155,  0.06604766, -0.05136783,\n",
       "        -0.10780494,  0.0615685 ,  0.06477591,  0.09894469, -0.07653868],\n",
       "       [-0.0788763 , -0.07045375,  0.09391812,  0.06994903, -0.06640016,\n",
       "        -0.12501805,  0.06565871,  0.06959901,  0.10147493, -0.05985155],\n",
       "       [-0.09914116, -0.06646591,  0.09244171,  0.06863186, -0.07849361,\n",
       "        -0.10561463,  0.06451783,  0.06785613,  0.09975437, -0.04348659],\n",
       "       [ 0.00130232, -0.09693945,  0.05559783,  0.04411389, -0.07725141,\n",
       "        -0.03856119,  0.04149078,  0.04515832,  0.06098738, -0.03589846],\n",
       "       [-0.03138412, -0.03672195,  0.04617456,  0.03554936, -0.09599915,\n",
       "        -0.04322909,  0.0329267 ,  0.03524966,  0.05092479,  0.00650925],\n",
       "       [-0.07247301,  0.00972694,  0.05156303,  0.03785332, -0.08804367,\n",
       "        -0.03844862,  0.03444709,  0.03706854,  0.05712668, -0.02882029],\n",
       "       [-0.01088935, -0.0339403 ,  0.04083759,  0.03084946, -0.02296916,\n",
       "        -0.061162  ,  0.02872743,  0.03127674,  0.04454868, -0.0472791 ],\n",
       "       [-0.05894407,  0.00053646,  0.05736251,  0.0423128 , -0.09235158,\n",
       "        -0.09598715,  0.03885677,  0.04120515,  0.06290023,  0.00410887],\n",
       "       [-0.04633321, -0.0636482 ,  0.08970135,  0.06767429, -0.08515023,\n",
       "        -0.11719142,  0.06286886,  0.06798354,  0.09809218, -0.07399715],\n",
       "       [-0.10400917, -0.05752139,  0.08383602,  0.06147508, -0.04401482,\n",
       "        -0.1042279 ,  0.05812157,  0.06052408,  0.08974141, -0.04392489],\n",
       "       [-0.09123808, -0.04111249,  0.0693861 ,  0.05103868, -0.04561758,\n",
       "        -0.04849749,  0.04784103,  0.05064629,  0.07505965, -0.06750611],\n",
       "       [-0.06242001, -0.05029557,  0.06496818,  0.04828942, -0.05366417,\n",
       "        -0.0949803 ,  0.04553683,  0.04761518,  0.06983532, -0.01488487],\n",
       "       [-0.12403563, -0.05583815,  0.08535999,  0.06259727, -0.06033209,\n",
       "        -0.07171438,  0.05907741,  0.061534  ,  0.09167124, -0.04831966],\n",
       "       [-0.04741232, -0.02125371,  0.05828142,  0.04351106, -0.07267357,\n",
       "        -0.0730769 ,  0.04014448,  0.04321275,  0.06396111, -0.03469432],\n",
       "       [-0.08022041, -0.03763814,  0.07693183,  0.05670102, -0.06456614,\n",
       "        -0.11105031,  0.05304533,  0.05584652,  0.08312466, -0.03217437],\n",
       "       [-0.03542776, -0.01147181,  0.04734397,  0.03537929, -0.06217478,\n",
       "        -0.05597427,  0.03241703,  0.03527307,  0.05229009, -0.03765482],\n",
       "       [-0.0214094 , -0.0026429 ,  0.04291253,  0.03236795, -0.07740349,\n",
       "        -0.06231505,  0.02933045,  0.03221987,  0.04788412, -0.02094408],\n",
       "       [-0.09325491, -0.06201093,  0.07908057,  0.05793267, -0.02617177,\n",
       "        -0.11098004,  0.05503278,  0.05708047,  0.08422683, -0.04093568],\n",
       "       [-0.0067875 , -0.10676579,  0.05954372,  0.04700052, -0.063046  ,\n",
       "        -0.02024816,  0.04430305,  0.04839936,  0.06521238, -0.06761159],\n",
       "       [-0.07128407, -0.0441757 ,  0.07715146,  0.0573726 , -0.06360282,\n",
       "        -0.07912151,  0.05341227,  0.05725574,  0.08403752, -0.07104548],\n",
       "       [-0.08469706, -0.07112811,  0.07319665,  0.05529715, -0.10550004,\n",
       "        -0.05421127,  0.05212314,  0.05446953,  0.07922116,  0.00122886],\n",
       "       [-0.06011496, -0.04989972,  0.0921989 ,  0.06908631, -0.10435477,\n",
       "        -0.14484144,  0.06416118,  0.06858897,  0.1005677 , -0.03539217],\n",
       "       [-0.0940006 , -0.05028643,  0.06839129,  0.0501956 , -0.04420125,\n",
       "        -0.07387893,  0.04756508,  0.04924686,  0.07312491, -0.02615652],\n",
       "       [-0.07902419, -0.04012384,  0.06566803,  0.04814438, -0.04657971,\n",
       "        -0.10200918,  0.04548941,  0.04705404,  0.07022885, -0.00884779],\n",
       "       [ 0.00026509, -0.05158153,  0.03616411,  0.02819821, -0.0414374 ,\n",
       "        -0.05057837,  0.02650468,  0.02860026,  0.03941721, -0.01555225],\n",
       "       [-0.05291309, -0.04489015,  0.05928213,  0.04498097, -0.07945582,\n",
       "        -0.0216572 ,  0.0416791 ,  0.04521177,  0.06531154, -0.05754926],\n",
       "       [-0.07559832, -0.06643447,  0.08372232,  0.06254999, -0.06063988,\n",
       "        -0.07798607,  0.05860299,  0.0625871 ,  0.09082867, -0.07763234],\n",
       "       [-0.07301912, -0.06027764,  0.05742804,  0.042913  , -0.05713093,\n",
       "        -0.05210201,  0.04083211,  0.04215676,  0.06145048, -0.00225069],\n",
       "       [-0.13900181, -0.02941764,  0.0845741 ,  0.06083223, -0.06829077,\n",
       "        -0.14734018,  0.05767752,  0.05817736,  0.08972514,  0.03306407],\n",
       "       [-0.04969399, -0.08325495,  0.05598483,  0.04175219, -0.00779313,\n",
       "        -0.09777762,  0.04050492,  0.0412111 ,  0.0586248 ,  0.00044187],\n",
       "       [-0.07466245, -0.09297614,  0.08841484,  0.06711116, -0.11112326,\n",
       "        -0.09178007,  0.06324392,  0.06658308,  0.09569853, -0.01050962],\n",
       "       [-0.12834993,  0.01730171,  0.07463278,  0.053802  , -0.09959564,\n",
       "        -0.06435731,  0.04940985,  0.05216515,  0.08168652, -0.03669512]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = np.dot(z1.T, dy)    \n",
    "dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터의 기울기 구하기\n",
    "dW2 = np.dot(z1.T, dy)\n",
    "db2 = np.sum(dy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간에 sigmoid가 한번 사용되이ㅓㅆ으므로, 활성화함수에 대한 gradient도 고려\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz1 = np.dot(dy, W2.T)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dW1 = np.dot(X.T, da1)\n",
    "db1 = np.sum(dz1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 오차역전파법\n",
    " 출력층의 결과와 내가 뽑고자 하는 target 값과의 차이를 구한 뒤, 그 오차값을 각 레이어들을 지나며 역전파해가며 각 노드가 가지고 있는 변수들을 갱신해 나가는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_backward(dy, cache):\n",
    "    X, W, b = cache\n",
    "    dX = np.dot(dy, W.T)\n",
    "    dW = np.dot(X.T, dy)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05699076 0.07709802 0.13266616 0.08636175 0.15977549 0.13191561\n",
      "  0.07067469 0.1164158  0.06519432 0.1029074 ]\n",
      " [0.05881493 0.06746346 0.10453152 0.09523672 0.16616693 0.12287557\n",
      "  0.08487497 0.1078604  0.07171387 0.12046163]\n",
      " [0.06155202 0.05921833 0.12657835 0.1025123  0.15401448 0.11871731\n",
      "  0.0901839  0.10309371 0.0755541  0.1085755 ]\n",
      " [0.04772839 0.06962345 0.12647524 0.09290889 0.15048205 0.12535483\n",
      "  0.08281817 0.1201921  0.05910023 0.12531666]\n",
      " [0.05361779 0.06202825 0.14284336 0.09693978 0.14567766 0.12834467\n",
      "  0.08245937 0.10575103 0.07234995 0.10998814]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.320339547462479\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# Forward Propagation\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "\n",
    "# 추론과 오차(Loss) 계산\n",
    "y_hat = softmax(a2)\n",
    "t = _change_ont_hot_label(Y_digit, 10)   # 정답 One-hot 인코딩\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "print(y_hat)\n",
    "print(t)\n",
    "print('Loss: ', Loss)\n",
    "        \n",
    "dy = (y_hat - t) / X.shape[0]\n",
    "dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "\n",
    "# 경사하강법을 통한 파라미터 업데이트    \n",
    "learning_rate = 0.1\n",
    "W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "    z1 = sigmoid(a1)\n",
    "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "    y_hat = softmax(a2)\n",
    "    t = _change_ont_hot_label(Y, 10)\n",
    "    Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "    if verbose:\n",
    "        print('---------')\n",
    "        print(y_hat)\n",
    "        print(t)\n",
    "        print('Loss: ', Loss)\n",
    "        \n",
    "    dy = (y_hat - t) / X.shape[0]\n",
    "    dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "    da1 = sigmoid_grad(a1) * dz1\n",
    "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "    \n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "[[0.0552363  0.06824488 0.06045284 0.04871128 0.22277953 0.06492689\n",
      "  0.08035606 0.1800316  0.05590786 0.16335275]\n",
      " [0.05046855 0.07439738 0.06620984 0.06546154 0.20457022 0.05909326\n",
      "  0.07679634 0.17286567 0.06494943 0.16518776]\n",
      " [0.0621766  0.06684455 0.07991139 0.05664445 0.21590521 0.07316118\n",
      "  0.09366768 0.13876218 0.06823088 0.14469588]\n",
      " [0.05338089 0.07166793 0.0679416  0.04674548 0.2197488  0.07328255\n",
      "  0.08941646 0.17685725 0.06185091 0.13910812]\n",
      " [0.05104303 0.07351477 0.06441476 0.05317175 0.17811762 0.07070521\n",
      "  0.07890238 0.21638996 0.06279201 0.15094851]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.3560684951034454\n",
      "---------\n",
      "[[0.0725755  0.08700549 0.05539366 0.04572903 0.21732357 0.08781475\n",
      "  0.0709962  0.13630428 0.05145115 0.17540639]\n",
      " [0.06938446 0.09378441 0.060943   0.06130692 0.19965504 0.07653959\n",
      "  0.06837063 0.13172976 0.06002901 0.17825719]\n",
      " [0.0770451  0.08125868 0.07267342 0.05238618 0.2207515  0.09003618\n",
      "  0.08271693 0.10696749 0.06260678 0.15355775]\n",
      " [0.06901873 0.0951555  0.06191436 0.04398208 0.21485368 0.09387359\n",
      "  0.07903352 0.13548153 0.05691355 0.14977348]\n",
      " [0.06528926 0.09145954 0.05912194 0.05047638 0.17618079 0.08925882\n",
      "  0.07029841 0.16867343 0.05832145 0.17091998]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.1460277122152474\n",
      "---------\n",
      "[[0.08977843 0.10431783 0.04978412 0.04185692 0.20737109 0.11128422\n",
      "  0.06199408 0.10769595 0.04637572 0.17954165]\n",
      " [0.09006969 0.11144377 0.05506718 0.05605349 0.19094016 0.09313205\n",
      "  0.06025983 0.10492285 0.05441241 0.18369856]\n",
      " [0.09143314 0.09435347 0.06543795 0.04775217 0.22265795 0.10560738\n",
      "  0.07279112 0.08607346 0.05680158 0.15709178]\n",
      " [0.08429638 0.11899444 0.05537601 0.04041619 0.20551925 0.1129916\n",
      "  0.06907956 0.10818283 0.05132984 0.1538139 ]\n",
      " [0.07917714 0.10765158 0.0532491  0.04677842 0.17047978 0.10625887\n",
      "  0.06191441 0.13646316 0.05309133 0.18493621]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.984276165295962\n",
      "---------\n",
      "[[0.10595612 0.11923502 0.04435972 0.03782321 0.19669766 0.13403891\n",
      "  0.05398049 0.08747784 0.04140304 0.17902798]\n",
      " [0.11173933 0.12645007 0.04933118 0.05061579 0.18161168 0.10790267\n",
      "  0.05299161 0.0859542  0.04887208 0.18453139]\n",
      " [0.10472386 0.10556189 0.05877073 0.04326288 0.2242435  0.11913315\n",
      "  0.06419962 0.07123854 0.05134604 0.15751979]\n",
      " [0.09843906 0.14200031 0.0491332  0.03670332 0.19541037 0.12954215\n",
      "  0.06023628 0.08875614 0.04587702 0.15390216]\n",
      " [0.09218075 0.12147753 0.04760648 0.04286687 0.16401503 0.12096345\n",
      "  0.05442713 0.11323165 0.0479335  0.19529762]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.856278207325874\n",
      "---------\n",
      "[[0.12044892 0.13132855 0.03943907 0.0339995  0.18684929 0.15518615\n",
      "  0.0471014  0.0725463  0.03685766 0.17624315]\n",
      " [0.13361353 0.13837795 0.04406465 0.04545439 0.17293465 0.12028695\n",
      "  0.04668653 0.07186196 0.04375675 0.18296264]\n",
      " [0.11644011 0.11463709 0.05286406 0.03915039 0.22668868 0.13023516\n",
      "  0.0569117  0.06020625 0.04644651 0.15642008]\n",
      " [0.11089251 0.16342143 0.04352346 0.03318079 0.18606498 0.14300188\n",
      "  0.05265872 0.07432701 0.04090489 0.15202432]\n",
      " [0.10387337 0.13268252 0.0425092  0.03912334 0.15800895 0.13300623\n",
      "  0.04798215 0.09579776 0.04319828 0.20381819]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.7524122447111374\n"
     ]
    }
   ],
   "source": [
    "X = x_train_reshaped[:5]\n",
    "Y = y_train[:5]\n",
    "\n",
    "# train_step을 다섯 번 반복 돌립니다.\n",
    "for i in range(5):\n",
    "    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 추론 과정 구현과 정확도 계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W1, b1, W2, b2, X):\n",
    "    a1 = np.dot(X, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13290335, 0.14065916, 0.03512943, 0.03055283, 0.17836142,\n",
       "       0.1743541 , 0.04130808, 0.06121107, 0.03285719, 0.17266338])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = x_train[:100] 에 대해 모델 추론을 시도합니다. \n",
    "\n",
    "X = x_train_reshaped[:100]\n",
    "Y = y_test[:100]\n",
    "result = predict(W1, b1, W2, b2, X)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W1, b1, W2, b2, x, y):\n",
    "    y_hat = predict(W1, b1, W2, b2, x)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "   # t = np.argmax(t, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13290335 0.14065916 0.03512943 0.03055283 0.17836142 0.1743541\n",
      " 0.04130808 0.06121107 0.03285719 0.17266338]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "0.09\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(W1, b1, W2, b2, X, Y)\n",
    "\n",
    "t = _change_ont_hot_label(Y, 10)\n",
    "print(result[0])\n",
    "print(t[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 전체 학습 사이클 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "\n",
    "    W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "    b1 = np.zeros(hidden_size)\n",
    "    W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "    b2 = np.zeros(output_size)\n",
    "\n",
    "    print(W1.shape)\n",
    "    print(b1.shape)\n",
    "    print(W2.shape)\n",
    "    print(b2.shape)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(50, 10)\n",
      "(10,)\n",
      "Loss:  2.3044652445878393\n",
      "train acc, test acc | 0.09736666666666667, 0.0982\n",
      "Loss:  0.7744872573015007\n",
      "train acc, test acc | 0.7874, 0.7944\n",
      "Loss:  0.3937297596900746\n",
      "train acc, test acc | 0.8779333333333333, 0.8818\n",
      "Loss:  0.2623987987387577\n",
      "train acc, test acc | 0.89785, 0.9021\n",
      "Loss:  0.29981509784212973\n",
      "train acc, test acc | 0.9085666666666666, 0.9098\n",
      "Loss:  0.19102788704774665\n",
      "train acc, test acc | 0.9155166666666666, 0.9192\n",
      "Loss:  0.17035630899838594\n",
      "train acc, test acc | 0.92025, 0.9244\n",
      "Loss:  0.3204519637436092\n",
      "train acc, test acc | 0.9237166666666666, 0.9271\n",
      "Loss:  0.2645219580592717\n",
      "train acc, test acc | 0.9272, 0.9309\n",
      "Loss:  0.30366193742571207\n",
      "train acc, test acc | 0.9308833333333333, 0.9331\n",
      "Loss:  0.31446298164167574\n",
      "train acc, test acc | 0.9346333333333333, 0.9357\n",
      "Loss:  0.19021138678707566\n",
      "train acc, test acc | 0.93725, 0.9378\n",
      "Loss:  0.26490538784495316\n",
      "train acc, test acc | 0.9399166666666666, 0.9403\n",
      "Loss:  0.22487685138249972\n",
      "train acc, test acc | 0.94195, 0.9413\n",
      "Loss:  0.17382197620887357\n",
      "train acc, test acc | 0.9436, 0.9432\n",
      "Loss:  0.20114042833297638\n",
      "train acc, test acc | 0.9455166666666667, 0.9447\n",
      "Loss:  0.18209714890254503\n",
      "train acc, test acc | 0.9477833333333333, 0.9468\n",
      "Loss:  0.09172847708483772\n",
      "train acc, test acc | 0.9495166666666667, 0.9478\n",
      "Loss:  0.21637700494778425\n",
      "train acc, test acc | 0.9505666666666667, 0.9487\n",
      "Loss:  0.21413427984718783\n",
      "train acc, test acc | 0.9518, 0.9497\n",
      "Loss:  0.10317900841499855\n",
      "train acc, test acc | 0.9532, 0.9508\n",
      "Loss:  0.1466363438008284\n",
      "train acc, test acc | 0.95385, 0.9524\n",
      "Loss:  0.13917253169138433\n",
      "train acc, test acc | 0.9550833333333333, 0.9524\n",
      "Loss:  0.1889959772310111\n",
      "train acc, test acc | 0.9558, 0.9535\n",
      "Loss:  0.15437244691389318\n",
      "train acc, test acc | 0.9574333333333334, 0.955\n",
      "Loss:  0.17170122221880543\n",
      "train acc, test acc | 0.9582833333333334, 0.9548\n",
      "Loss:  0.1859527756910339\n",
      "train acc, test acc | 0.9594833333333334, 0.9559\n",
      "Loss:  0.09987646923154672\n",
      "train acc, test acc | 0.9599666666666666, 0.9558\n",
      "Loss:  0.16077836244765653\n",
      "train acc, test acc | 0.9610166666666666, 0.9569\n",
      "Loss:  0.18071247856323847\n",
      "train acc, test acc | 0.9617666666666667, 0.9566\n",
      "Loss:  0.17703642788037058\n",
      "train acc, test acc | 0.9624666666666667, 0.9582\n",
      "Loss:  0.26453830530913597\n",
      "train acc, test acc | 0.9637666666666667, 0.9588\n",
      "Loss:  0.12119452590946604\n",
      "train acc, test acc | 0.9642, 0.9583\n",
      "Loss:  0.048189313820407566\n",
      "train acc, test acc | 0.9650166666666666, 0.9598\n",
      "Loss:  0.11988321536519943\n",
      "train acc, test acc | 0.9657, 0.9605\n",
      "Loss:  0.15499060542883752\n",
      "train acc, test acc | 0.9661, 0.9605\n",
      "Loss:  0.1158493532218304\n",
      "train acc, test acc | 0.9673333333333334, 0.9611\n",
      "Loss:  0.07183801676532661\n",
      "train acc, test acc | 0.96725, 0.9615\n",
      "Loss:  0.04727664918999142\n",
      "train acc, test acc | 0.96795, 0.9619\n",
      "Loss:  0.14504249565321595\n",
      "train acc, test acc | 0.96835, 0.9629\n",
      "Loss:  0.10673527560280871\n",
      "train acc, test acc | 0.9692833333333334, 0.9627\n",
      "Loss:  0.08882984853740777\n",
      "train acc, test acc | 0.9698, 0.9638\n",
      "Loss:  0.12175914901759916\n",
      "train acc, test acc | 0.9699166666666666, 0.964\n",
      "Loss:  0.14329935759196677\n",
      "train acc, test acc | 0.9704333333333334, 0.9643\n",
      "Loss:  0.04153837467341675\n",
      "train acc, test acc | 0.9716333333333333, 0.9649\n",
      "Loss:  0.11200959389837412\n",
      "train acc, test acc | 0.9712, 0.9655\n",
      "Loss:  0.07014444574561417\n",
      "train acc, test acc | 0.9724, 0.9656\n",
      "Loss:  0.17244229187820526\n",
      "train acc, test acc | 0.9726333333333333, 0.9655\n",
      "Loss:  0.11920529759953574\n",
      "train acc, test acc | 0.97335, 0.9663\n",
      "Loss:  0.04249880971393104\n",
      "train acc, test acc | 0.9737, 0.9659\n",
      "Loss:  0.08849894999079506\n",
      "train acc, test acc | 0.9738333333333333, 0.9667\n",
      "Loss:  0.08521558945165586\n",
      "train acc, test acc | 0.9745333333333334, 0.9662\n",
      "Loss:  0.12422898993977587\n",
      "train acc, test acc | 0.97505, 0.9663\n",
      "Loss:  0.07296708033824187\n",
      "train acc, test acc | 0.9751833333333333, 0.9671\n",
      "Loss:  0.10239021760189891\n",
      "train acc, test acc | 0.9757666666666667, 0.9678\n",
      "Loss:  0.06781500185101905\n",
      "train acc, test acc | 0.9760666666666666, 0.9671\n",
      "Loss:  0.11643209193446877\n",
      "train acc, test acc | 0.97635, 0.9675\n",
      "Loss:  0.06617032482915014\n",
      "train acc, test acc | 0.9765666666666667, 0.9678\n",
      "Loss:  0.16157669407307018\n",
      "train acc, test acc | 0.9768166666666667, 0.9675\n",
      "Loss:  0.04513617246825932\n",
      "train acc, test acc | 0.9771333333333333, 0.9681\n",
      "Loss:  0.08506312848882336\n",
      "train acc, test acc | 0.9772333333333333, 0.9683\n",
      "Loss:  0.08520508537361986\n",
      "train acc, test acc | 0.9781833333333333, 0.9683\n",
      "Loss:  0.09843766981821746\n",
      "train acc, test acc | 0.9782833333333333, 0.9688\n",
      "Loss:  0.05821781971509402\n",
      "train acc, test acc | 0.9782, 0.9689\n",
      "Loss:  0.07740829723902543\n",
      "train acc, test acc | 0.9784666666666667, 0.9697\n",
      "Loss:  0.11240159184558748\n",
      "train acc, test acc | 0.97885, 0.9706\n",
      "Loss:  0.07655850226332025\n",
      "train acc, test acc | 0.9793, 0.97\n",
      "Loss:  0.0767690657454543\n",
      "train acc, test acc | 0.9795, 0.97\n",
      "Loss:  0.08698037219847152\n",
      "train acc, test acc | 0.9794333333333334, 0.9704\n",
      "Loss:  0.04597948631291416\n",
      "train acc, test acc | 0.9799166666666667, 0.9697\n",
      "Loss:  0.0463943502204785\n",
      "train acc, test acc | 0.98015, 0.9708\n",
      "Loss:  0.14316756436425535\n",
      "train acc, test acc | 0.9802, 0.9707\n",
      "Loss:  0.07224312785628835\n",
      "train acc, test acc | 0.9807, 0.9704\n",
      "Loss:  0.07915762046108213\n",
      "train acc, test acc | 0.9808, 0.9706\n",
      "Loss:  0.05984199456440773\n",
      "train acc, test acc | 0.981, 0.9703\n",
      "Loss:  0.03375732409848565\n",
      "train acc, test acc | 0.9812833333333333, 0.9705\n",
      "Loss:  0.06517005309564247\n",
      "train acc, test acc | 0.9815166666666667, 0.9717\n",
      "Loss:  0.07687218488171126\n",
      "train acc, test acc | 0.9818666666666667, 0.9714\n",
      "Loss:  0.09074942558045353\n",
      "train acc, test acc | 0.9818, 0.971\n",
      "Loss:  0.04995114762880963\n",
      "train acc, test acc | 0.98185, 0.9715\n",
      "Loss:  0.05153828875361687\n",
      "train acc, test acc | 0.9823, 0.9716\n",
      "Loss:  0.07193640927363448\n",
      "train acc, test acc | 0.9821166666666666, 0.9711\n",
      "Loss:  0.025240003365401532\n",
      "train acc, test acc | 0.9823666666666667, 0.9721\n",
      "Loss:  0.08469509112769524\n",
      "train acc, test acc | 0.9828166666666667, 0.9715\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 50000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "W1, b1, W2, b2 = init_params(784, 50, 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train_reshaped[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    W1, b1, W2, b2, Loss = train_step(x_batch, y_batch, W1, b1, W2, b2, learning_rate=0.1, verbose=False)\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    train_loss_list.append(Loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        print('Loss: ', Loss)\n",
    "        train_acc = accuracy(W1, b1, W2, b2, x_train_reshaped, y_train)\n",
    "        test_acc = accuracy(W1, b1, W2, b2, x_test_reshaped, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 6 \n",
    "\n",
    "# Accuracy 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss 그래프 그리기\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list, label='train acc')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 3.0)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
